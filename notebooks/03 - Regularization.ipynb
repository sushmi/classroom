{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "Regularization is a umbrella term that refers to approaches that help reduce the chance of overfitting, usually either by normalization or through added penatlies.\n",
    "\n",
    "### 1. Ridge regression ($L_2$ regularization)\n",
    "\n",
    "Perhaps the most common form of regularization is known as *ridge regression* or $L_2$ *regularization*, sometimes also called *Tikhonov regularization*. This proceeds by penalizing the sum of squares (2-norms) of the model coefficients; in this case, the penalty on the model fit would be \n",
    "\n",
    "$$ J(\\theta) =  \\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda\\sum_{j=1}^n \\boldsymbol{\\theta}_j^2$$\n",
    "\n",
    "where $\\lambda$ is a free parameter that controls the strength of the penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lasso regression ($L_1$ regularization)\n",
    "\n",
    "Another very common type of regularization is known as lasso, and involves penalizing the sum of absolute values (1-norms) of regression coefficients:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2+ \\lambda\\sum_{j=1}^n |\\theta_j|$$\n",
    "\n",
    "Though this is conceptually very similar to ridge regression, the results can differ surprisingly: that is, lasso regression can set certain features to zero, while ridge regression focuses on shrinking all parameters so it is not overly reliant on one feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Elastic net \n",
    "\n",
    "When you lack domain knowledge, you can simply use elastic net.\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{j=1}^n |\\theta_j| + (1 - \\lambda) \\sum_{k=1}^n \\theta_k^2\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge or Lasso or Elastic net??\n",
    "\n",
    "Regularization should be ALMOST ALWAYS used, since these techniques reduces overfitting.\n",
    "\n",
    "How to choose is a little bit difficult. It is easier to understand the assumptions behind.\n",
    "1.  Ridge assumes that coefficients are normally distributed.   **Thus, if you don't want any feature to dominate too much, use Ridge.**\n",
    "2. Lasso assumes that coefficients are Laplace distributed (in layman sense, it mean some predictors are very useful while some are completely irrelevant).   Here, Lasso has the ability to shrink coefficient to zero thus eliminate predictors that are not useful to the output, thus automatic feature selection.  **In simple words, if you have only very few predictors with medium/large effect, use Lasso.**\n",
    "3.  Elastic basically is a compromise between the two, and thus take huge computation time to reach that compromise.  **If you have the resource to spare, you can use Elastic net**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "MlflowException",
     "evalue": "API request to endpoint /api/2.0/mlflow/experiments/get-by-name failed with error code 403 != 200. Response body: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOGNAME\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchaky\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# mlflow.create_experiment(name=\"chaky-diabetes-example\")  #create if you haven't create\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchaky-regularization-example\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/ml/Python-for-Machine-Learning/.venv/lib/python3.13/site-packages/mlflow/tracking/fluent.py:183\u001b[0m, in \u001b[0;36mset_experiment\u001b[0;34m(experiment_name, experiment_id)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _experiment_lock:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m experiment_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m         experiment \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment:\n\u001b[1;32m    185\u001b[0m             _logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    186\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperiment with name \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not exist. Creating a new experiment.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    187\u001b[0m                 experiment_name,\n\u001b[1;32m    188\u001b[0m             )\n",
      "File \u001b[0;32m~/code/ml/Python-for-Machine-Learning/.venv/lib/python3.13/site-packages/mlflow/tracking/_tracking_service/client.py:261\u001b[0m, in \u001b[0;36mTrackingServiceClient.get_experiment_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_experiment_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[1;32m    254\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m        name: The experiment name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;124;03m        :py:class:`mlflow.entities.Experiment`\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/ml/Python-for-Machine-Learning/.venv/lib/python3.13/site-packages/mlflow/store/tracking/rest_store.py:756\u001b[0m, in \u001b[0;36mRestStore.get_experiment_by_name\u001b[0;34m(self, experiment_name)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    755\u001b[0m     req_body \u001b[38;5;241m=\u001b[39m message_to_json(GetExperimentByName(experiment_name\u001b[38;5;241m=\u001b[39mexperiment_name))\n\u001b[0;32m--> 756\u001b[0m     response_proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGetExperimentByName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Experiment\u001b[38;5;241m.\u001b[39mfrom_proto(response_proto\u001b[38;5;241m.\u001b[39mexperiment)\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MlflowException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/code/ml/Python-for-Machine-Learning/.venv/lib/python3.13/site-packages/mlflow/store/tracking/rest_store.py:133\u001b[0m, in \u001b[0;36mRestStore._call_endpoint\u001b[0;34m(self, api, json_body, endpoint, retry_timeout_seconds)\u001b[0m\n\u001b[1;32m    131\u001b[0m     endpoint, method \u001b[38;5;241m=\u001b[39m _METHOD_TO_INFO[api]\n\u001b[1;32m    132\u001b[0m response_proto \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mResponse()\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_host_creds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_proto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_timeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_timeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/ml/Python-for-Machine-Learning/.venv/lib/python3.13/site-packages/mlflow/utils/rest_utils.py:553\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers, retry_timeout_seconds)\u001b[0m\n\u001b[1;32m    550\u001b[0m     call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m json_body\n\u001b[1;32m    551\u001b[0m     response \u001b[38;5;241m=\u001b[39m http_request(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcall_kwargs)\n\u001b[0;32m--> 553\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mverify_rest_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    554\u001b[0m response_to_parse \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/code/ml/Python-for-Machine-Learning/.venv/lib/python3.13/site-packages/mlflow/utils/rest_utils.py:314\u001b[0m, in \u001b[0;36mverify_rest_response\u001b[0;34m(response, endpoint)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    310\u001b[0m         base_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    311\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI request to endpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed with error code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != 200\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m         )\n\u001b[0;32m--> 314\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    315\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Response body: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    316\u001b[0m             error_code\u001b[38;5;241m=\u001b[39mget_error_code(response\u001b[38;5;241m.\u001b[39mstatus_code),\n\u001b[1;32m    317\u001b[0m         )\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# Skip validation for endpoints (e.g. DBFS file-download API) which may return a non-JSON\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# response\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m endpoint\u001b[38;5;241m.\u001b[39mstartswith(_REST_API_PATH_PREFIX) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _can_parse_as_json_object(response\u001b[38;5;241m.\u001b[39mtext):\n",
      "\u001b[0;31mMlflowException\u001b[0m: API request to endpoint /api/2.0/mlflow/experiments/get-by-name failed with error code 403 != 200. Response body: ''"
     ]
    }
   ],
   "source": [
    "#experiment tracking\n",
    "import mlflow\n",
    "import os\n",
    "# This the dockerized method.\n",
    "# We build two docker containers, one for python/jupyter and another for mlflow.\n",
    "# The url `mlflow` is resolved into another container within the same composer.\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "# In the dockerized way, the user who runs this code will be `root`.\n",
    "# The MLflow will also log the run user_id as `root`.\n",
    "# To change that, we need to set this environ[\"LOGNAME\"] to your name.\n",
    "os.environ[\"LOGNAME\"] = \"chaky\"\n",
    "# mlflow.create_experiment(name=\"chaky-diabetes-example\")  #create if you haven't create\n",
    "mlflow.set_experiment(experiment_name=\"chaky-regularization-example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "print(\"Features: \", diabetes.feature_names)\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "m = X.shape[0]  #number of samples\n",
    "n = X.shape[1]  #number of features\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "# actually you can do like this too\n",
    "# X = np.insert(X, 0, 1, axis=1)\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "X_train   = np.concatenate((intercept, X_train), axis=1)\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "X_test    = np.concatenate((intercept, X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class LinearRegression(object):\n",
    "    \n",
    "    #in this class, we add cross validation as well for some spicy code....\n",
    "    kfold = KFold(n_splits=3)\n",
    "            \n",
    "    def __init__(self, regularization, lr=0.001, method='batch', num_epochs=500, batch_size=50, cv=kfold):\n",
    "        self.lr         = lr\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.method     = method\n",
    "        self.cv         = cv\n",
    "        self.regularization = regularization\n",
    "\n",
    "    def mse(self, ytrue, ypred):\n",
    "        return ((ypred - ytrue) ** 2).sum() / ytrue.shape[0]\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "            \n",
    "        #create a list of kfold scores\n",
    "        self.kfold_scores = list()\n",
    "        \n",
    "        #reset val loss\n",
    "        self.val_loss_old = np.infty\n",
    "\n",
    "        #kfold.split in the sklearn.....\n",
    "        #5 splits\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv.split(X_train)):\n",
    "            \n",
    "            X_cross_train = X_train[train_idx]\n",
    "            y_cross_train = y_train[train_idx]\n",
    "            X_cross_val   = X_train[val_idx]\n",
    "            y_cross_val   = y_train[val_idx]\n",
    "            \n",
    "            self.theta = np.zeros(X_cross_train.shape[1])\n",
    "            \n",
    "            #define X_cross_train as only a subset of the data\n",
    "            #how big is this subset?  => mini-batch size ==> 50\n",
    "            \n",
    "            #one epoch will exhaust the WHOLE training set\n",
    "            with mlflow.start_run(run_name=f\"Fold-{fold}\", nested=True):\n",
    "                \n",
    "                params = {\"method\": self.method, \"lr\": self.lr, \"reg\": type(self).__name__}\n",
    "                mlflow.log_params(params=params)\n",
    "                \n",
    "                for epoch in range(self.num_epochs):\n",
    "                \n",
    "                    #with replacement or no replacement\n",
    "                    #with replacement means just randomize\n",
    "                    #with no replacement means 0:50, 51:100, 101:150, ......300:323\n",
    "                    #shuffle your index\n",
    "                    perm = np.random.permutation(X_cross_train.shape[0])\n",
    "                            \n",
    "                    X_cross_train = X_cross_train[perm]\n",
    "                    y_cross_train = y_cross_train[perm]\n",
    "                    \n",
    "                    if self.method == 'sto':\n",
    "                        for batch_idx in range(X_cross_train.shape[0]):\n",
    "                            X_method_train = X_cross_train[batch_idx].reshape(1, -1) #(11,) ==> (1, 11) ==> (m, n)\n",
    "                            y_method_train = y_cross_train[batch_idx] \n",
    "                            train_loss = self._train(X_method_train, y_method_train)\n",
    "                    elif self.method == 'mini':\n",
    "                        for batch_idx in range(0, X_cross_train.shape[0], self.batch_size):\n",
    "                            #batch_idx = 0, 50, 100, 150\n",
    "                            X_method_train = X_cross_train[batch_idx:batch_idx+self.batch_size, :]\n",
    "                            y_method_train = y_cross_train[batch_idx:batch_idx+self.batch_size]\n",
    "                            train_loss = self._train(X_method_train, y_method_train)\n",
    "                    else:\n",
    "                        X_method_train = X_cross_train\n",
    "                        y_method_train = y_cross_train\n",
    "                        train_loss = self._train(X_method_train, y_method_train)\n",
    "\n",
    "                    mlflow.log_metric(key=\"train_loss\", value=train_loss, step=epoch)\n",
    "\n",
    "                    yhat_val = self.predict(X_cross_val)\n",
    "                    val_loss_new = self.mse(y_cross_val, yhat_val)\n",
    "                    mlflow.log_metric(key=\"val_loss\", value=val_loss_new, step=epoch)\n",
    "                    \n",
    "                    #early stopping\n",
    "                    if np.allclose(val_loss_new, self.val_loss_old):\n",
    "                        break\n",
    "                    self.val_loss_old = val_loss_new\n",
    "            \n",
    "                self.kfold_scores.append(val_loss_new)\n",
    "                print(f\"Fold {fold}: {val_loss_new}\")\n",
    "            \n",
    "                    \n",
    "    def _train(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        m    = X.shape[0]        \n",
    "        grad = (1/m) * X.T @(yhat - y) + self.regularization.derivation(self.theta)\n",
    "        self.theta = self.theta - self.lr * grad\n",
    "        return self.mse(y, yhat)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self.theta  #===>(m, n) @ (n, )\n",
    "    \n",
    "    def _coef(self):\n",
    "        return self.theta[1:]  #remind that theta is (w0, w1, w2, w3, w4.....wn)\n",
    "                               #w0 is the bias or the intercept\n",
    "                               #w1....wn are the weights / coefficients / theta\n",
    "    def _bias(self):\n",
    "        return self.theta[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create `Ridge`, `Lasso` and `Elastic` class that extends the `LinearRegression`, with added penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoPenalty:\n",
    "    \n",
    "    def __init__(self, l):\n",
    "        self.l = l # lambda value\n",
    "        \n",
    "    def __call__(self, theta): #__call__ allows us to call class as method\n",
    "        return self.l * np.sum(np.abs(theta))\n",
    "        \n",
    "    def derivation(self, theta):\n",
    "        return self.l * np.sign(theta)\n",
    "    \n",
    "class RidgePenalty:\n",
    "    \n",
    "    def __init__(self, l):\n",
    "        self.l = l\n",
    "        \n",
    "    def __call__(self, theta): #__call__ allows us to call class as method\n",
    "        return self.l * np.sum(np.square(theta))\n",
    "        \n",
    "    def derivation(self, theta):\n",
    "        return self.l * 2 * theta\n",
    "    \n",
    "class ElasticPenalty:\n",
    "    \n",
    "    def __init__(self, l = 0.1, l_ratio = 0.5):\n",
    "        self.l = l \n",
    "        self.l_ratio = l_ratio\n",
    "\n",
    "    def __call__(self, theta):  #__call__ allows us to call class as method\n",
    "        l1_contribution = self.l_ratio * self.l * np.sum(np.abs(theta))\n",
    "        l2_contribution = (1 - self.l_ratio) * self.l * 0.5 * np.sum(np.square(theta))\n",
    "        return (l1_contribution + l2_contribution)\n",
    "\n",
    "    def derivation(self, theta):\n",
    "        l1_derivation = self.l * self.l_ratio * np.sign(theta)\n",
    "        l2_derivation = self.l * (1 - self.l_ratio) * theta\n",
    "        return (l1_derivation + l2_derivation)\n",
    "    \n",
    "class Lasso(LinearRegression):\n",
    "    \n",
    "    def __init__(self, method, lr, l):\n",
    "        self.regularization = LassoPenalty(l)\n",
    "        super().__init__(self.regularization, lr, method)\n",
    "        \n",
    "class Ridge(LinearRegression):\n",
    "    \n",
    "    def __init__(self, method, lr, l):\n",
    "        self.regularization = RidgePenalty(l)\n",
    "        super().__init__(self.regularization, lr, method)\n",
    "        \n",
    "class ElasticNet(LinearRegression):\n",
    "    \n",
    "    def __init__(self, method, lr, l, l_ratio=0.5):\n",
    "        self.regularization = ElasticPenalty(l, l_ratio)\n",
    "        super().__init__(self.regularization, lr, method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for looping classnames\n",
    "import sys\n",
    "\n",
    "def str_to_class(classname):\n",
    "    return getattr(sys.modules[__name__], classname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Ridge =====\n",
      "Fold 0: 5019.957764397\n",
      "Fold 1: 3789.7283075525406\n",
      "Fold 2: 3231.9871784874213\n",
      "Test MSE:  3952.505584097506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaklam/Github/Machine-Learning/.venv/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Lasso =====\n",
      "Fold 0: 3473.727565365972\n",
      "Fold 1: 3417.382642134025\n",
      "Fold 2: 3204.376603720075\n",
      "Test MSE:  2914.256969310874\n",
      "===== ElasticNet =====\n",
      "Fold 0: 3784.5882498239384\n",
      "Fold 1: 3384.868051931774\n",
      "Fold 2: 3064.9560116605444\n",
      "Test MSE:  3065.0966947039674\n"
     ]
    }
   ],
   "source": [
    "regs = [\"Ridge\", \"Lasso\", \"ElasticNet\"]\n",
    "\n",
    "for reg in regs:\n",
    "\n",
    "    params = {\"method\": \"batch\", \"lr\": 0.1, \"l\": 0.1}\n",
    "    mlflow.start_run(run_name=f\"method-{params['method']}-lr-{params['lr']}-reg-{reg}\", nested=True)\n",
    "    \n",
    "    print(\"=\"*5, reg, \"=\"*5)\n",
    "\n",
    "    # #######\n",
    "    type_of_regression = str_to_class(reg)    #Ridge, Lasso, ElasticNet\n",
    "    model = type_of_regression(**params)  \n",
    "    model.fit(X_train, y_train)\n",
    "    yhat = model.predict(X_test)\n",
    "    mse  = model.mse(y_test, yhat)\n",
    "\n",
    "    print(\"Test MSE: \", mse)\n",
    "    mlflow.log_metric(key=\"test_mse\", value=mse)\n",
    "\n",
    "    signature = mlflow.models.infer_signature(X_train, model.predict(X_train))\n",
    "    mlflow.sklearn.log_model(model, artifact_path='model', signature=signature)\n",
    "\n",
    "    # #######\n",
    "\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Workshop - Check your understandings\n",
    "\n",
    "| | Egg price | Gold price    | Oil price   | GDP   | $\\hat{y}$ |  $(\\hat{y} - y)^2$ |\n",
    "|---:|:------ |:-----------   |:------      |:------|:------    |  :------           |    \n",
    "| 1  | 3      | 100           | 4           | 21    | 28        |  49                |\n",
    "| 2  | 4      | 500           | 7           | 43    | 79        |  1296              |\n",
    "| 3  | 5      | 200           | 6           | 56    | 48        |  64                |\n",
    "| 4  | 6      | 300           | 8           | 21    | 66        |  2025              |\n",
    "| 5  | 7      | 400           | 2           | 44    | 60        |  256               |\n",
    "\n",
    "Note:  this table is filled with given weight of [2, 0.1, 3].\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "Instruction:  Gather in your group.  Will randomly pick groups to present.\n",
    "\n",
    "1.  Explain regularization in your own words.\n",
    "2.  For `LassoPenalty`, show by hand why the derivation is `self.l * np.sign(theta)`\n",
    "3.  For `RidgePenalty`, show by hand why the derivation is `self.l * 2 * theta`\n",
    "4.  Using mini-batch gradient descent based on $(\\mathbf{x}^{(2)}, \\mathbf{x}^{(4)}, \\mathbf{x}^{(5)})$, and using **Ridge regression**, what is $J(\\theta)$?  Assume $\\lambda = 0.1$\n",
    "5.  Continued from above, what is $\\displaystyle \\frac{\\partial J}{\\partial \\theta_2}$\n",
    "6.  Continued from above, what is $\\displaystyle \\frac{\\partial J}{\\partial \\theta}$\n",
    "7.  What would be the new theta?\n",
    "8.  Performed 4,5,6 with **Lasso Regression**.  Assume $\\lambda = 0.1$\n",
    "9.  What does `LinearRegression` inside `Ridge(LinearRegression)` mean?\n",
    "10. What does `str_to_class` function do?\n",
    "11. Based on everything you have done in this workshop, what do you think is a good way to decide when to use Ridge, Lasso, or Elastic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "classroom (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
